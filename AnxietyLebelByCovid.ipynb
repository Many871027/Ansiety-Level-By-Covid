{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scikit_posthocs as sp\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from random import randint, uniform\n",
    "from scipy.stats import randint as sp_randint, uniform\n",
    "import warnings\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib  \n",
    "import os  \n",
    "import plotly.express as px\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold  # Import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "\n",
    "\n",
    "# --- Variable Encoding Function ---\n",
    "def encode_variables(df):  \n",
    "    \"\"\"\n",
    "    Encodes the DataFrame variables according to the specified plan.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame with the original data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with encoded variables.\n",
    "    \"\"\"\n",
    "\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    # 1. anxiety_level: Ordinal Encoding (0, 1, 2, 3)\n",
    "    anxiety_mapping = {\n",
    "        \"minimal\": 0,\n",
    "        \"mild\": 1,\n",
    "        \"moderate\": 2,\n",
    "        \"severe\": 3,\n",
    "    }\n",
    "    df_encoded[\"anxiety_level\"] = df_encoded[\"anxiety_level\"].map(anxiety_mapping)\n",
    "\n",
    "    # 2. sex: Binary Encoding (0 and 1)\n",
    "    sex_mapping = {\"female\": 0, \"male\": 1}\n",
    "    df_encoded[\"sex\"] = df_encoded[\"sex\"].map(sex_mapping)\n",
    "\n",
    "    # 3. education_level: Ordinal Encoding (0, 1, 2)\n",
    "    education_mapping = {\n",
    "        \"technical level\": 0,\n",
    "        \"bachelor\": 1,\n",
    "        \"graduate\": 2,\n",
    "    }\n",
    "    df_encoded[\"education_level\"] = df_encoded[\"education_level\"].map(\n",
    "        education_mapping\n",
    "    )\n",
    "\n",
    "    # 4. shift: One-Hot Encoding\n",
    "    df_encoded = pd.get_dummies(\n",
    "        df_encoded, columns=[\"shift\"], prefix=\"shift\", dummy_na=False\n",
    "    )\n",
    "\n",
    "    # 5. marital_status: One-Hot Encoding + Handling 'widowed'\n",
    "    df_encoded[\"marital_status\"] = df_encoded[\"marital_status\"].replace(\n",
    "        \"widowed\", \"single\"\n",
    "    )  # Group with 'single'\n",
    "    df_encoded = pd.get_dummies(\n",
    "        df_encoded, columns=[\"marital_status\"], prefix=\"marital\", dummy_na=False\n",
    "    )\n",
    "\n",
    "    # 6. category: One-Hot Encoding + Grouping\n",
    "    df_encoded[\"category\"] = df_encoded[\"category\"].replace(\n",
    "        [\"spec nurse\", \"head nurse\"], \"other\"\n",
    "    )  # Group\n",
    "    df_encoded = pd.get_dummies(\n",
    "        df_encoded, columns=[\"category\"], prefix=\"category\", dummy_na=False\n",
    "    )\n",
    "\n",
    "    # 7. age_range: Ordinal Encoding\n",
    "    age_categories = [\"20 to 29\", \"30 to 39\", \"40 to 49\", \"50 and over\"]\n",
    "    age_encoder = OrdinalEncoder(categories=[age_categories])\n",
    "    df_encoded[\"age_range\"] = age_encoder.fit_transform(\n",
    "        df_encoded[[\"age_range\"]]\n",
    "    )\n",
    "    df_encoded[\"age_range\"] = df_encoded[\"age_range\"].astype(int)\n",
    "\n",
    "    # 8. seniority_range: Ordinal Encoding\n",
    "    seniority_categories = [\n",
    "        \"1 to 5\",\n",
    "        \"6 to 10\",\n",
    "        \"11 to 15\",\n",
    "        \"16 to 20\",\n",
    "        \"21 and over\",\n",
    "    ]\n",
    "    seniority_encoder = OrdinalEncoder(categories=[seniority_categories])\n",
    "    df_encoded[\"seniority_range\"] = seniority_encoder.fit_transform(\n",
    "        df_encoded[[\"seniority_range\"]]\n",
    "    )\n",
    "    df_encoded[\"seniority_range\"] = df_encoded[\"seniority_range\"].astype(int)\n",
    "\n",
    "  # --- CREATION OF INTERACTION VARIABLES ---\n",
    "\n",
    "    # 1. Interactions between numerical/ordinal variables:\n",
    "    df_encoded['age_x_seniority'] = df_encoded['age_range'] * df_encoded['seniority_range']\n",
    "    df_encoded['age_x_education'] = df_encoded['age_range'] * df_encoded['education_level']\n",
    "    df_encoded['seniority_x_education'] = df_encoded['seniority_range'] * df_encoded['education_level']\n",
    "\n",
    "    # 2. Interactions between sex and other variables:\n",
    "    df_encoded['sex_x_age'] = df_encoded['sex'] * df_encoded['age_range']\n",
    "    df_encoded['sex_x_seniority'] = df_encoded['sex'] * df_encoded['seniority_range']\n",
    "    df_encoded['sex_x_education'] = df_encoded['sex'] * df_encoded['education_level']\n",
    "\n",
    "    # 3. Interactions between one-hot encoded variables and numerical/ordinal variables:\n",
    "    for col in ['shift_afternoon', 'shift_morning', 'shift_night a', 'shift_night b',\n",
    "                'marital_domestic partnership', 'marital_married', 'marital_single',\n",
    "                'category_gen nurse', 'category_nurse aux', 'category_other']:\n",
    "        df_encoded[f'{col}_x_age'] = df_encoded[col] * df_encoded['age_range']\n",
    "        df_encoded[f'{col}_x_seniority'] = df_encoded[col] * df_encoded['seniority_range']\n",
    "        df_encoded[f'{col}_x_education'] = df_encoded[col] * df_encoded['education_level']\n",
    "\n",
    "\n",
    "    # 4. Interactions between one-hot encoded variables (categorical):\n",
    "    for col1 in ['shift_afternoon', 'shift_morning', 'shift_night a', 'shift_night b']:\n",
    "        for col2 in ['marital_domestic partnership', 'marital_married', 'marital_single']:\n",
    "            df_encoded[f'{col1}_x_{col2}'] = df_encoded[col1] * df_encoded[col2]\n",
    "\n",
    "    for col1 in ['shift_afternoon', 'shift_morning', 'shift_night a', 'shift_night b']:\n",
    "        for col2 in ['category_gen nurse', 'category_nurse aux', 'category_other']:\n",
    "            df_encoded[f'{col1}_x_{col2}'] = df_encoded[col1] * df_encoded[col2]\n",
    "\n",
    "    for col1 in ['marital_domestic partnership', 'marital_married', 'marital_single']:\n",
    "        for col2 in ['category_gen nurse', 'category_nurse aux', 'category_other']:\n",
    "            df_encoded[f'{col1}_x_{col2}'] = df_encoded[col1] * df_encoded[col2]\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "\n",
    "# --- Data Augmentation Function with SMOTE ---\n",
    "def augment_data_smote(\n",
    "    df, target_variable, sampling_strategy=\"auto\", random_state=None, k_neighbors=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies SMOTE to oversample the target variable, with improved error handling\n",
    "    and k_neighbors adjustment.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame with the encoded data.\n",
    "        target_variable: Name of the column containing the (categorical) target variable.\n",
    "        sampling_strategy: Sampling strategy.\n",
    "        random_state: Random seed.\n",
    "        k_neighbors: Number of neighbors.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame, Series: DataFrame with augmented predictor variables and Series with the\n",
    "                         augmented target variable.  Or None, None if there's an error.\n",
    "    \"\"\"\n",
    "\n",
    "    X = df.drop(target_variable, axis=1)\n",
    "    y = df[target_variable]\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(y):\n",
    "        print(\"Error: The target variable must be numeric (ordinally encoded).\")\n",
    "        return None, None\n",
    "\n",
    "    if len(y.unique()) < 2:\n",
    "        print(\"Error: The target variable must have at least two different classes.\")\n",
    "        return None, None\n",
    "\n",
    "    class_counts = Counter(y)\n",
    "    min_samples = min(class_counts.values())\n",
    "\n",
    "    if k_neighbors is None:\n",
    "        k_neighbors = min(5, min_samples - 1)\n",
    "    else:\n",
    "        k_neighbors = min(k_neighbors, min_samples - 1)\n",
    "\n",
    "    if k_neighbors < 1:\n",
    "        print(\"Error: At least one class has very few samples. SMOTE cannot be applied.\")\n",
    "        print(\"Consider grouping minority classes or removing the solitary sample.\")\n",
    "        return None, None\n",
    "\n",
    "    if min_samples <= 1:\n",
    "        print(\"Error: At least one class has only one sample. SMOTE cannot be applied.\")\n",
    "        print(\"Consider grouping minority classes or removing the solitary sample before applying SMOTE\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Using k_neighbors = {k_neighbors} for SMOTE.\")\n",
    "\n",
    "    try:\n",
    "        smote = SMOTE(\n",
    "            sampling_strategy=sampling_strategy,\n",
    "            random_state=random_state,\n",
    "            k_neighbors=k_neighbors,\n",
    "        )\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error applying SMOTE: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    print(\"Class distribution before SMOTE:\", Counter(y))\n",
    "    print(\"Class distribution after SMOTE:\", Counter(y_resampled))\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "def analyze_spearman_correlation(df, target_variable):\n",
    "    \"\"\"\n",
    "    Calculates and visualizes Spearman correlations between the (ordinal) target variable\n",
    "    and other numerical/ordinal variables in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame with the data (already augmented and encoded).\n",
    "        target_variable: Name of the column containing the (ordinal) target variable.\n",
    "\n",
    "    Returns:\n",
    "        None (prints the correlation matrix and shows a heatmap).\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the Spearman correlation matrix\n",
    "    spearman_correlations = df.corr(method=\"spearman\")\n",
    "\n",
    "    # Extract correlations with the target variable\n",
    "    correlations_with_target = spearman_correlations[target_variable].drop(\n",
    "        target_variable\n",
    "    )  # Exclude self-correlation\n",
    "\n",
    "    # Print correlations with the target variable\n",
    "    print(\"Spearman Correlations with\", target_variable, \":\\n\")\n",
    "    print(correlations_with_target.sort_values(ascending=False))\n",
    "\n",
    "    # Visualize with a heatmap (all variables)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(spearman_correlations, annot=True, cmap=\"coolwarm\", center=0)\n",
    "    plt.title(\"Spearman Correlation Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"spearman_correlation_heatmap.png\")  # Save the figure\n",
    "    plt.close() #Close figure\n",
    "\n",
    "    # Visualize with a bar plot (only correlations with the target variable)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    correlations_with_target.sort_values().plot(kind=\"barh\", color=\"skyblue\")\n",
    "    plt.title(f\"Spearman Correlation with {target_variable}\")\n",
    "    plt.xlabel(\"Spearman Correlation Coefficient\")\n",
    "    plt.ylabel(\"Variables\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"spearman_correlation_barplot.png\")  # Save the figure\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def inferential_analysis(df, target_variable):\n",
    "    \"\"\"\n",
    "    Performs Kruskal-Wallis and Chi-square tests to analyze relationships\n",
    "    between the target variable and other variables in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame with the data (already augmented and encoded).\n",
    "        target_variable: Name of the column containing the (ordinal) target variable.\n",
    "\n",
    "    Returns:\n",
    "        None (prints the test results).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Kruskal-Wallis (for numerical/ordinal variables) ---\n",
    "    print(\"-\" * 50)\n",
    "    print(\"KRUSKAL-WALLIS TESTS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # List of numerical/ordinal variables (excluding target and one-hot encoded)\n",
    "    numerical_ordinal_vars = [\n",
    "        col\n",
    "        for col in df.columns\n",
    "        if (\n",
    "            pd.api.types.is_numeric_dtype(df[col])\n",
    "            and col != target_variable\n",
    "            and not col.startswith((\"shift_\", \"marital_\", \"category_\"))\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for variable in numerical_ordinal_vars:\n",
    "        # Group data by levels of the target variable\n",
    "        groups = [\n",
    "            df[variable][df[target_variable] == level]\n",
    "            for level in df[target_variable].unique()\n",
    "        ]\n",
    "\n",
    "        # Check for at least two groups and non-empty groups\n",
    "        if len(groups) < 2:\n",
    "            print(\n",
    "                f\"Cannot perform Kruskal-Wallis on {variable}: fewer than two groups.\"\n",
    "            )\n",
    "            continue\n",
    "        if any(len(group) == 0 for group in groups):\n",
    "            print(\n",
    "                f\"Cannot perform Kruskal-Wallis on {variable}: at least one group is empty.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Perform Kruskal-Wallis test\n",
    "        try:\n",
    "            statistic, p_value = stats.kruskal(*groups)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error performing Kruskal-Wallis on {variable}: {e}\")\n",
    "            print(\"This may occur if all values in a group are equal.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nKruskal-Wallis: {variable} vs. {target_variable}\")\n",
    "        print(f\"  H Statistic: {statistic:.3f}\")\n",
    "        print(f\"  p-value: {p_value:.3f}\")\n",
    "\n",
    "        # Interpretation\n",
    "        if p_value < 0.05:\n",
    "            print(\n",
    "                f\"  Result: Significant differences between levels of {target_variable} on variable {variable}.\"\n",
    "            )\n",
    "\n",
    "            # --- Post-Hoc Tests (Dunn with Bonferroni/Holm correction) ---\n",
    "            # Dunn's test\n",
    "            dunn_result = sp.posthoc_dunn(\n",
    "                a=df,\n",
    "                val_col=variable,\n",
    "                group_col=target_variable,\n",
    "                p_adjust=\"bonferroni\",  # Or 'holm'\n",
    "            )\n",
    "            print(\"\\n  Dunn's Test (Post-Hoc):\")\n",
    "            print(dunn_result)\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                f\"  Result: No significant differences between levels of {target_variable} on variable {variable}.\"\n",
    "            )\n",
    "\n",
    "    # --- 2. Chi-square (for categorical variables) ---\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"CHI-SQUARE TESTS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # List of categorical variables (including one-hot encoded)\n",
    "    categorical_vars = [\n",
    "        col\n",
    "        for col in df.columns\n",
    "        if col\n",
    "        in (\n",
    "            \"sex\",\n",
    "            \"shift_afternoon\",\n",
    "            \"shift_morning\",\n",
    "            \"shift_night a\",\n",
    "            \"shift_night b\",\n",
    "            \"marital_domestic partnership\",\n",
    "            \"marital_married\",\n",
    "            \"marital_single\",\n",
    "            \"category_gen nurse\",\n",
    "            \"category_nurse aux\",\n",
    "            \"category_other\",\n",
    "            \"education_level\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for variable in categorical_vars:\n",
    "        # Create contingency table\n",
    "        contingency_table = pd.crosstab(df[target_variable], df[variable])\n",
    "        print(f\"\\nContingency Table: {target_variable} vs. {variable}\")\n",
    "        print(contingency_table)\n",
    "\n",
    "        # Perform Chi-square test\n",
    "        try:\n",
    "            chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "            print(f\"\\nChi-square: {variable} vs. {target_variable}\")\n",
    "            print(f\"  Chi2 Statistic: {chi2:.3f}\")\n",
    "            print(f\"  p-value: {p_value:.3f}\")\n",
    "            print(f\"  Degrees of freedom: {dof}\")\n",
    "\n",
    "            # Interpretation\n",
    "            if p_value < 0.05:\n",
    "                print(\n",
    "                    f\"  Result: Significant association between {target_variable} and {variable}.\"\n",
    "                )\n",
    "                # Calculate Cramer's V\n",
    "                n = contingency_table.sum().sum()\n",
    "                phi2 = chi2 / n\n",
    "                r, k = contingency_table.shape\n",
    "                phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "                rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "                kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "                cramers_v = (phi2corr / min((kcorr - 1), (rcorr - 1))) ** 0.5\n",
    "                print(f\"  Cramer's V: {cramers_v:.3f}\")\n",
    "\n",
    "            else:\n",
    "                print(\n",
    "                    f\"  Result: No significant association between {target_variable} and {variable}.\"\n",
    "                )\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error performing Chi-square on {variable}: {e}\")\n",
    "            print(\"This may occur if any expected frequencies are zero.\")\n",
    "            continue\n",
    "\n",
    "def train_model(model, X_train, y_train):\n",
    "    \"\"\"Trains a given model.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Evaluates a model and prints metrics.  Returns test accuracy.\"\"\"\n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    conf_matrix = confusion_matrix(y_test, model.predict(X_test))\n",
    "\n",
    "    # Handle multiclass classification report\n",
    "    try:\n",
    "        class_report = classification_report(y_test, model.predict(X_test))\n",
    "    except ValueError as e:\n",
    "        print(f\"Error generating classification report: {e}\")\n",
    "        print(\"This usually happens if the predicted labels don't cover all classes present in y_test.\")\n",
    "        print(\"Check if your model is predicting all classes, especially after SMOTE and train/test split.\")\n",
    "        class_report = \"Could not generate report (see error above)\"\n",
    "\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
    "    print(f\"Classification Report:\\n{class_report}\")\n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Evaluates a model and prints metrics.  Returns test accuracy.\"\"\"\n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    conf_matrix = confusion_matrix(y_test, model.predict(X_test))\n",
    "\n",
    "    # Handle multiclass classification report\n",
    "    try:\n",
    "        class_report = classification_report(y_test, model.predict(X_test))\n",
    "    except ValueError as e:\n",
    "        print(f\"Error generating classification report: {e}\")\n",
    "        print(\"This usually happens if the predicted labels don't cover all classes present in y_test.\")\n",
    "        print(\"Check if your model is predicting all classes, especially after SMOTE and train/test split.\")\n",
    "        class_report = \"Could not generate report (see error above)\"\n",
    "\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
    "    print(f\"Classification Report:\\n{class_report}\")\n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def optimize_and_evaluate(model_name, model, param_dist, X_train, y_train, X_test, y_test, n_iter=20, cv=5):\n",
    "    \"\"\"Performs RandomizedSearchCV, saves/loads results, evaluates, and returns test accuracy.\"\"\"\n",
    "\n",
    "    save_path = f\"{model_name}_random_search.pkl\"  # Simplified path\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(save_path):\n",
    "            random_search = joblib.load(save_path)\n",
    "            print(f\"Loaded RandomizedSearchCV for {model_name} from {save_path}\")\n",
    "        else:\n",
    "            print(f\"No checkpoint found for {model_name}. Starting RandomizedSearchCV.\")\n",
    "            print(f\"Type of param_dist: {type(param_dist)}\")  # Debug: Check type\n",
    "            print(f\"Contents of param_dist: {param_dist}\") # Debug: Check contents\n",
    "\n",
    "            random_search = RandomizedSearchCV(\n",
    "                model,\n",
    "                param_dist,\n",
    "                n_iter=n_iter,\n",
    "                cv=cv,\n",
    "                scoring='accuracy',\n",
    "                n_jobs=-1,\n",
    "                random_state=42,\n",
    "                verbose=0\n",
    "            )\n",
    "            random_search.fit(X_train, y_train)\n",
    "            joblib.dump(random_search, save_path)  # Save after fitting\n",
    "            print(f\"RandomizedSearchCV for {model_name} saved to {save_path}\")\n",
    "\n",
    "\n",
    "        best_model = random_search.best_estimator_\n",
    "        print(f\"Best parameters for {model_name}: {random_search.best_params_}\")\n",
    "        test_accuracy = evaluate_model(best_model, X_train, y_train, X_test, y_test)\n",
    "        return test_accuracy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in optimize_and_evaluate for {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_models_dict():\n",
    "    \"\"\"Creates the dictionary of models with pipelines for scaling.\"\"\"\n",
    "    models = {\n",
    "        \"KNN\": Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())]),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "        \"Gradient Boosting\": Pipeline([('scaler', StandardScaler()), ('gb', GradientBoostingClassifier(random_state=42))]),\n",
    "        \"XGBoost\": Pipeline([('scaler', StandardScaler()), ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'))]), #Multiclass classification\n",
    "        \"Extra Trees\": ExtraTreesClassifier(random_state=42),\n",
    "    }\n",
    "    return models\n",
    "\n",
    "\n",
    "def create_param_distributions():\n",
    "    \"\"\"Defines parameter distributions for RandomizedSearchCV.\"\"\"\n",
    "    param_distributions = {\n",
    "        \"KNN\": {\n",
    "            'knn__n_neighbors': sp_randint(3, 200),  \n",
    "            'knn__weights': ['uniform', 'distance'],\n",
    "            'knn__p': [1, 2]\n",
    "        },\n",
    "        \"Decision Tree\": {\n",
    "            'max_depth': sp_randint(3, 200),  \n",
    "            'min_samples_split': sp_randint(2, 200), \n",
    "            'min_samples_leaf': sp_randint(1, 200),  \n",
    "            'criterion': ['gini', 'entropy', \"log_loss\"]\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            'n_estimators': sp_randint(100, 5001), \n",
    "            'max_depth': sp_randint(3, 31),\n",
    "            'min_samples_split': sp_randint(2, 41),\n",
    "            'min_samples_leaf': sp_randint(1, 41),\n",
    "            'max_features': [None, 'sqrt', 'log2'],\n",
    "            'bootstrap': [True, False]\n",
    "        },\n",
    "        \"AdaBoost\": {\n",
    "            'n_estimators': sp_randint(50, 1001), \n",
    "            'learning_rate': uniform(0.001, 0.9)  \n",
    "        },\n",
    "        \"Gradient Boosting\": {\n",
    "            'gb__n_estimators': sp_randint(100, 5001), \n",
    "            'gb__learning_rate': uniform(0.001, 0.5),  \n",
    "            'gb__max_depth': sp_randint(3, 64), \n",
    "            'gb__min_samples_split': sp_randint(2, 21),\n",
    "            'gb__min_samples_leaf': sp_randint(1, 21),\n",
    "            'gb__subsample': uniform(0.6, 0.8),  \n",
    "            'gb__max_features': [None, 'sqrt', 'log2']\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            'xgb__n_estimators': sp_randint(100, 5001), \n",
    "            'xgb__learning_rate': uniform(0.001, 0.6),  \n",
    "            'xgb__max_depth': sp_randint(3, 64), \n",
    "            'xgb__subsample': uniform(0.6, 0.2),  \n",
    "            'xgb__colsample_bytree': uniform(0.6, 0.2),\n",
    "            'xgb__gamma': uniform(0, 0.5)\n",
    "        },\n",
    "        \"Extra Trees\": {\n",
    "            'n_estimators': sp_randint(100, 5001), \n",
    "            'max_depth': sp_randint(3, 31),\n",
    "            'min_samples_split': sp_randint(2, 41),\n",
    "            'min_samples_leaf': sp_randint(1, 41),\n",
    "            'max_features': [None, 'sqrt', 'log2'],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "    }\n",
    "    return param_distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "df = pd.read_csv(r\"D:\\ansiedad\\AnxietyLevelByCovid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Encode Variables\n",
    "df_encoded = encode_variables(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using k_neighbors = 5 for SMOTE.\n",
      "Class distribution before SMOTE: Counter({0: 62, 1: 49, 2: 22, 3: 7})\n",
      "Class distribution after SMOTE: Counter({1: 100, 0: 100, 2: 100, 3: 100})\n"
     ]
    }
   ],
   "source": [
    "# 3. Augment Data\n",
    "sampling_strategy = {\n",
    "    0: 100,  # minimal\n",
    "    1: 100,  # mild\n",
    "    2: 100,  # moderate\n",
    "    3: 100,  # severe\n",
    "}\n",
    "\n",
    "X_resampled, y_resampled = augment_data_smote(\n",
    "    df_encoded,\n",
    "    target_variable=\"anxiety_level\",\n",
    "    sampling_strategy=sampling_strategy,\n",
    "    random_state=42,\n",
    "    #k_neighbors=3,  # Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmented DataFrame:\n",
      "   sex  education_level  age_range  seniority_range  shift_afternoon  \\\n",
      "0    0                1          0                0            False   \n",
      "1    0                2          2                3            False   \n",
      "2    1                2          1                2            False   \n",
      "3    0                1          2                3            False   \n",
      "4    0                2          1                2            False   \n",
      "\n",
      "   shift_morning  shift_night a  shift_night b  marital_domestic partnership  \\\n",
      "0          False           True          False                          True   \n",
      "1           True          False          False                         False   \n",
      "2           True          False          False                         False   \n",
      "3           True          False          False                          True   \n",
      "4           True          False          False                         False   \n",
      "\n",
      "   marital_married  ...  marital_domestic partnership_x_category_gen nurse  \\\n",
      "0            False  ...                                               True   \n",
      "1             True  ...                                              False   \n",
      "2            False  ...                                              False   \n",
      "3            False  ...                                              False   \n",
      "4            False  ...                                              False   \n",
      "\n",
      "   marital_domestic partnership_x_category_nurse aux  \\\n",
      "0                                              False   \n",
      "1                                              False   \n",
      "2                                              False   \n",
      "3                                              False   \n",
      "4                                              False   \n",
      "\n",
      "   marital_domestic partnership_x_category_other  \\\n",
      "0                                          False   \n",
      "1                                          False   \n",
      "2                                          False   \n",
      "3                                           True   \n",
      "4                                          False   \n",
      "\n",
      "   marital_married_x_category_gen nurse  marital_married_x_category_nurse aux  \\\n",
      "0                                 False                                 False   \n",
      "1                                 False                                 False   \n",
      "2                                 False                                 False   \n",
      "3                                 False                                 False   \n",
      "4                                 False                                 False   \n",
      "\n",
      "   marital_married_x_category_other  marital_single_x_category_gen nurse  \\\n",
      "0                             False                                False   \n",
      "1                              True                                False   \n",
      "2                             False                                False   \n",
      "3                             False                                False   \n",
      "4                             False                                 True   \n",
      "\n",
      "   marital_single_x_category_nurse aux  marital_single_x_category_other  \\\n",
      "0                                False                            False   \n",
      "1                                False                            False   \n",
      "2                                False                             True   \n",
      "3                                False                            False   \n",
      "4                                False                            False   \n",
      "\n",
      "   anxiety_level  \n",
      "0              1  \n",
      "1              0  \n",
      "2              1  \n",
      "3              2  \n",
      "4              1  \n",
      "\n",
      "[5 rows x 84 columns]\n",
      "(400, 84)\n",
      "Spearman Correlations with anxiety_level :\n",
      "\n",
      "education_level                                  0.063957\n",
      "shift_night b_x_education                        0.053155\n",
      "shift_night b_x_seniority                        0.021915\n",
      "marital_single_x_education                       0.010306\n",
      "marital_domestic partnership_x_category_other    0.000000\n",
      "                                                   ...   \n",
      "shift_afternoon_x_age                           -0.223731\n",
      "age_range                                       -0.233366\n",
      "shift_afternoon_x_marital_single                -0.242115\n",
      "age_x_seniority                                 -0.244353\n",
      "seniority_range                                 -0.267716\n",
      "Name: anxiety_level, Length: 83, dtype: float64\n",
      "--------------------------------------------------\n",
      "KRUSKAL-WALLIS TESTS\n",
      "--------------------------------------------------\n",
      "\n",
      "Kruskal-Wallis: sex vs. anxiety_level\n",
      "  H Statistic: 16.436\n",
      "  p-value: 0.001\n",
      "  Result: Significant differences between levels of anxiety_level on variable sex.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2         3\n",
      "0  1.000000  0.302845  0.302845  1.000000\n",
      "1  0.302845  1.000000  0.000550  0.028349\n",
      "2  0.302845  0.000550  1.000000  1.000000\n",
      "3  1.000000  0.028349  1.000000  1.000000\n",
      "\n",
      "Kruskal-Wallis: education_level vs. anxiety_level\n",
      "  H Statistic: 2.777\n",
      "  p-value: 0.427\n",
      "  Result: No significant differences between levels of anxiety_level on variable education_level.\n",
      "\n",
      "Kruskal-Wallis: age_range vs. anxiety_level\n",
      "  H Statistic: 28.961\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable age_range.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "         0         1         2         3\n",
      "0  1.00000  1.000000  1.000000  0.000050\n",
      "1  1.00000  1.000000  1.000000  0.000009\n",
      "2  1.00000  1.000000  1.000000  0.003439\n",
      "3  0.00005  0.000009  0.003439  1.000000\n",
      "\n",
      "Kruskal-Wallis: seniority_range vs. anxiety_level\n",
      "  H Statistic: 44.452\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable seniority_range.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "              0             1         2             3\n",
      "0  1.000000e+00  1.000000e+00  1.000000  7.926546e-07\n",
      "1  1.000000e+00  1.000000e+00  1.000000  1.529628e-08\n",
      "2  1.000000e+00  1.000000e+00  1.000000  6.519396e-06\n",
      "3  7.926546e-07  1.529628e-08  0.000007  1.000000e+00\n",
      "\n",
      "Kruskal-Wallis: age_x_seniority vs. anxiety_level\n",
      "  H Statistic: 33.906\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable age_x_seniority.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2         3\n",
      "0  1.000000  1.000000  1.000000  0.000009\n",
      "1  1.000000  1.000000  1.000000  0.000002\n",
      "2  1.000000  1.000000  1.000000  0.000244\n",
      "3  0.000009  0.000002  0.000244  1.000000\n",
      "\n",
      "Kruskal-Wallis: age_x_education vs. anxiety_level\n",
      "  H Statistic: 10.501\n",
      "  p-value: 0.015\n",
      "  Result: Significant differences between levels of anxiety_level on variable age_x_education.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2         3\n",
      "0  1.000000  0.712344  1.000000  0.897169\n",
      "1  0.712344  1.000000  1.000000  0.016127\n",
      "2  1.000000  1.000000  1.000000  0.079535\n",
      "3  0.897169  0.016127  0.079535  1.000000\n",
      "\n",
      "Kruskal-Wallis: seniority_x_education vs. anxiety_level\n",
      "  H Statistic: 45.611\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable seniority_x_education.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0             1         2             3\n",
      "0  1.000000  2.420599e-01  1.000000  1.106397e-04\n",
      "1  0.242060  1.000000e+00  1.000000  1.440719e-09\n",
      "2  1.000000  1.000000e+00  1.000000  1.659283e-06\n",
      "3  0.000111  1.440719e-09  0.000002  1.000000e+00\n",
      "\n",
      "Kruskal-Wallis: sex_x_age vs. anxiety_level\n",
      "  H Statistic: 25.494\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable sex_x_age.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2         3\n",
      "0  1.000000  0.018482  1.000000  0.406425\n",
      "1  0.018482  1.000000  0.000936  0.000010\n",
      "2  1.000000  0.000936  1.000000  1.000000\n",
      "3  0.406425  0.000010  1.000000  1.000000\n",
      "\n",
      "Kruskal-Wallis: sex_x_seniority vs. anxiety_level\n",
      "  H Statistic: 26.234\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable sex_x_seniority.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2         3\n",
      "0  1.000000  0.003582  1.000000  0.799946\n",
      "1  0.003582  1.000000  0.003430  0.000005\n",
      "2  1.000000  0.003430  1.000000  0.818272\n",
      "3  0.799946  0.000005  0.818272  1.000000\n",
      "\n",
      "Kruskal-Wallis: sex_x_education vs. anxiety_level\n",
      "  H Statistic: 10.962\n",
      "  p-value: 0.012\n",
      "  Result: Significant differences between levels of anxiety_level on variable sex_x_education.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2        3\n",
      "0  1.000000  0.086362  1.000000  1.00000\n",
      "1  0.086362  1.000000  0.009724  0.29834\n",
      "2  1.000000  0.009724  1.000000  1.00000\n",
      "3  1.000000  0.298340  1.000000  1.00000\n",
      "\n",
      "--------------------------------------------------\n",
      "CHI-SQUARE TESTS\n",
      "--------------------------------------------------\n",
      "\n",
      "Contingency Table: anxiety_level vs. sex\n",
      "sex             0   1\n",
      "anxiety_level        \n",
      "0              87  13\n",
      "1              78  22\n",
      "2              96   4\n",
      "3              91   9\n",
      "\n",
      "Chi-square: sex vs. anxiety_level\n",
      "  Chi2 Statistic: 16.477\n",
      "  p-value: 0.001\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and sex.\n",
      "  Cramer's V: 0.184\n",
      "\n",
      "Contingency Table: anxiety_level vs. education_level\n",
      "education_level   0   1  2\n",
      "anxiety_level             \n",
      "0                25  72  3\n",
      "1                19  76  5\n",
      "2                20  78  2\n",
      "3                14  85  1\n",
      "\n",
      "Chi-square: education_level vs. anxiety_level\n",
      "  Chi2 Statistic: 7.452\n",
      "  p-value: 0.281\n",
      "  Degrees of freedom: 6\n",
      "  Result: No significant association between anxiety_level and education_level.\n",
      "\n",
      "Contingency Table: anxiety_level vs. shift_afternoon\n",
      "shift_afternoon  False  True \n",
      "anxiety_level                \n",
      "0                   63     37\n",
      "1                   72     28\n",
      "2                   87     13\n",
      "3                   77     23\n",
      "\n",
      "Chi-square: shift_afternoon vs. anxiety_level\n",
      "  Chi2 Statistic: 15.934\n",
      "  p-value: 0.001\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and shift_afternoon.\n",
      "  Cramer's V: 0.180\n",
      "\n",
      "Contingency Table: anxiety_level vs. shift_morning\n",
      "shift_morning  False  True \n",
      "anxiety_level              \n",
      "0                 72     28\n",
      "1                 71     29\n",
      "2                 65     35\n",
      "3                 94      6\n",
      "\n",
      "Chi-square: shift_morning vs. anxiety_level\n",
      "  Chi2 Statistic: 26.220\n",
      "  p-value: 0.000\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and shift_morning.\n",
      "  Cramer's V: 0.241\n",
      "\n",
      "Contingency Table: anxiety_level vs. shift_night a\n",
      "shift_night a  False  True \n",
      "anxiety_level              \n",
      "0                 92      8\n",
      "1                 91      9\n",
      "2                 89     11\n",
      "3                100      0\n",
      "\n",
      "Chi-square: shift_night a vs. anxiety_level\n",
      "  Chi2 Statistic: 10.753\n",
      "  p-value: 0.013\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and shift_night a.\n",
      "  Cramer's V: 0.139\n",
      "\n",
      "Contingency Table: anxiety_level vs. shift_night b\n",
      "shift_night b  False  True \n",
      "anxiety_level              \n",
      "0                 86     14\n",
      "1                 93      7\n",
      "2                 98      2\n",
      "3                 98      2\n",
      "\n",
      "Chi-square: shift_night b vs. anxiety_level\n",
      "  Chi2 Statistic: 16.512\n",
      "  p-value: 0.001\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and shift_night b.\n",
      "  Cramer's V: 0.184\n",
      "\n",
      "Contingency Table: anxiety_level vs. marital_domestic partnership\n",
      "marital_domestic partnership  False  True \n",
      "anxiety_level                             \n",
      "0                                86     14\n",
      "1                                87     13\n",
      "2                                91      9\n",
      "3                               100      0\n",
      "\n",
      "Chi-square: marital_domestic partnership vs. anxiety_level\n",
      "  Chi2 Statistic: 14.896\n",
      "  p-value: 0.002\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and marital_domestic partnership.\n",
      "  Cramer's V: 0.173\n",
      "\n",
      "Contingency Table: anxiety_level vs. marital_married\n",
      "marital_married  False  True \n",
      "anxiety_level                \n",
      "0                   67     33\n",
      "1                   58     42\n",
      "2                   76     24\n",
      "3                   88     12\n",
      "\n",
      "Chi-square: marital_married vs. anxiety_level\n",
      "  Chi2 Statistic: 24.577\n",
      "  p-value: 0.000\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and marital_married.\n",
      "  Cramer's V: 0.233\n",
      "\n",
      "Contingency Table: anxiety_level vs. marital_single\n",
      "marital_single  False  True \n",
      "anxiety_level               \n",
      "0                  56     44\n",
      "1                  72     28\n",
      "2                  86     14\n",
      "3                  63     37\n",
      "\n",
      "Chi-square: marital_single vs. anxiety_level\n",
      "  Chi2 Statistic: 23.610\n",
      "  p-value: 0.000\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and marital_single.\n",
      "  Cramer's V: 0.227\n",
      "\n",
      "Contingency Table: anxiety_level vs. category_gen nurse\n",
      "category_gen nurse  False  True \n",
      "anxiety_level                   \n",
      "0                      55     45\n",
      "1                      41     59\n",
      "2                      57     43\n",
      "3                      76     24\n",
      "\n",
      "Chi-square: category_gen nurse vs. anxiety_level\n",
      "  Chi2 Statistic: 25.363\n",
      "  p-value: 0.000\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and category_gen nurse.\n",
      "  Cramer's V: 0.237\n",
      "\n",
      "Contingency Table: anxiety_level vs. category_nurse aux\n",
      "category_nurse aux  False  True \n",
      "anxiety_level                   \n",
      "0                      56     44\n",
      "1                      75     25\n",
      "2                      75     25\n",
      "3                      81     19\n",
      "\n",
      "Chi-square: category_nurse aux vs. anxiety_level\n",
      "  Chi2 Statistic: 17.502\n",
      "  p-value: 0.001\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and category_nurse aux.\n",
      "  Cramer's V: 0.191\n",
      "\n",
      "Contingency Table: anxiety_level vs. category_other\n",
      "category_other  False  True \n",
      "anxiety_level               \n",
      "0                  96      4\n",
      "1                  92      8\n",
      "2                  95      5\n",
      "3                 100      0\n",
      "\n",
      "Chi-square: category_other vs. anxiety_level\n",
      "  Chi2 Statistic: 8.048\n",
      "  p-value: 0.045\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and category_other.\n",
      "  Cramer's V: 0.112\n"
     ]
    }
   ],
   "source": [
    "# 4. Create Augmented DataFrame\n",
    "if X_resampled is not None and y_resampled is not None:\n",
    "    df_augmented = pd.DataFrame(X_resampled, columns=X_resampled.columns)\n",
    "    df_augmented[\"anxiety_level\"] = y_resampled\n",
    "\n",
    "    # --- Now work with df_augmented ---\n",
    "    print(\"\\nAugmented DataFrame:\")\n",
    "    print(df_augmented.head())\n",
    "    print(df_augmented.shape)\n",
    "\n",
    "    # --- Perform correlation analysis, inferential analysis, etc. ---\n",
    "    analyze_spearman_correlation(df_augmented, \"anxiety_level\")\n",
    "    inferential_analysis(df_augmented, \"anxiety_level\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Data augmentation failed. Check for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 84)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlations with anxiety_level :\n",
      "\n",
      "education_level                                  0.063957\n",
      "shift_night b_x_education                        0.053155\n",
      "shift_night b_x_seniority                        0.021915\n",
      "marital_single_x_education                       0.010306\n",
      "marital_domestic partnership_x_category_other    0.000000\n",
      "                                                   ...   \n",
      "shift_afternoon_x_age                           -0.223731\n",
      "age_range                                       -0.233366\n",
      "shift_afternoon_x_marital_single                -0.242115\n",
      "age_x_seniority                                 -0.244353\n",
      "seniority_range                                 -0.267716\n",
      "Name: anxiety_level, Length: 83, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "analyze_spearman_correlation(df_augmented, \"anxiety_level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman Correlation Analysis Summary\n",
    "\n",
    "This section summarizes the findings from the Spearman rank correlation analysis, focusing on the relationships between the `anxiety_level` (our ordinal target variable) and other predictor variables in the dataset (after data augmentation with SMOTE and feature engineering).\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "*   **Generally Weak Correlations:** Overall, most predictor variables exhibit weak correlations with `anxiety_level`, with the strongest correlation being -0.267716.  This reinforces that anxiety, as measured in this study, is a complex phenomenon and is likely influenced by a combination of factors, rather than any single strong predictor.\n",
    "\n",
    "*   **Negative Correlations (Weak):**\n",
    "    *   `seniority_range` (-0.267716): Shows the strongest negative correlation, suggesting that nurses with *more* experience tend to report *lower* anxiety levels.\n",
    "    *   `age_x_seniority` (-0.244353): An interaction term between age and seniority. The negative correlation suggests the combined effect of higher age and more seniority is associated with lower reported anxiety. This may largely reflect the `seniority_range` effect, as age and seniority are likely correlated.\n",
    "    *   `shift_afternoon_x_marital_single` (-0.242115):  An interaction term. Nurses who work the afternoon shift *and* are single tend to report slightly lower anxiety.\n",
    "    *  `age_range` (-0.233366):  Older nurses *tend* to report slightly lower anxiety levels.\n",
    "    *  `shift_afternoon_x_age` (-0.223731):  Nurses who work the afternoon shift and are in older age range *tend* to report slightly lower anxiety levels.\n",
    "\n",
    "*   **Positive Correlations (Very Weak):**\n",
    "    *   `education_level` (0.063957):  A very weak positive correlation, suggesting a slight, and potentially spurious, tendency for nurses with higher education levels to report *higher* anxiety.  Further investigation (hypothesis testing) is needed.\n",
    "    *   `shift_night b_x_education` (0.053155): A very weak positive correlation. Nurses working the \"night b\" shift *and* having higher education levels tend to report *slightly* higher anxiety.\n",
    "    *  `shift_night b_x_seniority` (0.021915) and `marital_single_x_education` (0.010306) show correlations very close to zero.\n",
    "\n",
    "*   **Multicollinearity:** While not shown in this specific output, remember that strong correlations *between predictor variables* (like `age_range` and `seniority_range`) indicate multicollinearity. This should be addressed in modeling (e.g., by using only one of the highly correlated variables, or creating a combined variable).\n",
    "\n",
    "* **Variables with very weak correlation (near zero)**\n",
    "    * Many variables, and specially interactions, show correlation very close to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "KRUSKAL-WALLIS TESTS\n",
      "--------------------------------------------------\n",
      "\n",
      "Kruskal-Wallis: sex vs. anxiety_level\n",
      "  H Statistic: 16.436\n",
      "  p-value: 0.001\n",
      "  Result: Significant differences between levels of anxiety_level on variable sex.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2         3\n",
      "0  1.000000  0.302845  0.302845  1.000000\n",
      "1  0.302845  1.000000  0.000550  0.028349\n",
      "2  0.302845  0.000550  1.000000  1.000000\n",
      "3  1.000000  0.028349  1.000000  1.000000\n",
      "\n",
      "Kruskal-Wallis: education_level vs. anxiety_level\n",
      "  H Statistic: 2.777\n",
      "  p-value: 0.427\n",
      "  Result: No significant differences between levels of anxiety_level on variable education_level.\n",
      "\n",
      "Kruskal-Wallis: age_range vs. anxiety_level\n",
      "  H Statistic: 28.961\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable age_range.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "         0         1         2         3\n",
      "0  1.00000  1.000000  1.000000  0.000050\n",
      "1  1.00000  1.000000  1.000000  0.000009\n",
      "2  1.00000  1.000000  1.000000  0.003439\n",
      "3  0.00005  0.000009  0.003439  1.000000\n",
      "\n",
      "Kruskal-Wallis: seniority_range vs. anxiety_level\n",
      "  H Statistic: 44.452\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable seniority_range.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "              0             1         2             3\n",
      "0  1.000000e+00  1.000000e+00  1.000000  7.926546e-07\n",
      "1  1.000000e+00  1.000000e+00  1.000000  1.529628e-08\n",
      "2  1.000000e+00  1.000000e+00  1.000000  6.519396e-06\n",
      "3  7.926546e-07  1.529628e-08  0.000007  1.000000e+00\n",
      "\n",
      "Kruskal-Wallis: age_x_seniority vs. anxiety_level\n",
      "  H Statistic: 33.906\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable age_x_seniority.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2         3\n",
      "0  1.000000  1.000000  1.000000  0.000009\n",
      "1  1.000000  1.000000  1.000000  0.000002\n",
      "2  1.000000  1.000000  1.000000  0.000244\n",
      "3  0.000009  0.000002  0.000244  1.000000\n",
      "\n",
      "Kruskal-Wallis: age_x_education vs. anxiety_level\n",
      "  H Statistic: 10.501\n",
      "  p-value: 0.015\n",
      "  Result: Significant differences between levels of anxiety_level on variable age_x_education.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2         3\n",
      "0  1.000000  0.712344  1.000000  0.897169\n",
      "1  0.712344  1.000000  1.000000  0.016127\n",
      "2  1.000000  1.000000  1.000000  0.079535\n",
      "3  0.897169  0.016127  0.079535  1.000000\n",
      "\n",
      "Kruskal-Wallis: seniority_x_education vs. anxiety_level\n",
      "  H Statistic: 45.611\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable seniority_x_education.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0             1         2             3\n",
      "0  1.000000  2.420599e-01  1.000000  1.106397e-04\n",
      "1  0.242060  1.000000e+00  1.000000  1.440719e-09\n",
      "2  1.000000  1.000000e+00  1.000000  1.659283e-06\n",
      "3  0.000111  1.440719e-09  0.000002  1.000000e+00\n",
      "\n",
      "Kruskal-Wallis: sex_x_age vs. anxiety_level\n",
      "  H Statistic: 25.494\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable sex_x_age.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2         3\n",
      "0  1.000000  0.018482  1.000000  0.406425\n",
      "1  0.018482  1.000000  0.000936  0.000010\n",
      "2  1.000000  0.000936  1.000000  1.000000\n",
      "3  0.406425  0.000010  1.000000  1.000000\n",
      "\n",
      "Kruskal-Wallis: sex_x_seniority vs. anxiety_level\n",
      "  H Statistic: 26.234\n",
      "  p-value: 0.000\n",
      "  Result: Significant differences between levels of anxiety_level on variable sex_x_seniority.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2         3\n",
      "0  1.000000  0.003582  1.000000  0.799946\n",
      "1  0.003582  1.000000  0.003430  0.000005\n",
      "2  1.000000  0.003430  1.000000  0.818272\n",
      "3  0.799946  0.000005  0.818272  1.000000\n",
      "\n",
      "Kruskal-Wallis: sex_x_education vs. anxiety_level\n",
      "  H Statistic: 10.962\n",
      "  p-value: 0.012\n",
      "  Result: Significant differences between levels of anxiety_level on variable sex_x_education.\n",
      "\n",
      "  Dunn's Test (Post-Hoc):\n",
      "          0         1         2        3\n",
      "0  1.000000  0.086362  1.000000  1.00000\n",
      "1  0.086362  1.000000  0.009724  0.29834\n",
      "2  1.000000  0.009724  1.000000  1.00000\n",
      "3  1.000000  0.298340  1.000000  1.00000\n",
      "\n",
      "--------------------------------------------------\n",
      "CHI-SQUARE TESTS\n",
      "--------------------------------------------------\n",
      "\n",
      "Contingency Table: anxiety_level vs. sex\n",
      "sex             0   1\n",
      "anxiety_level        \n",
      "0              87  13\n",
      "1              78  22\n",
      "2              96   4\n",
      "3              91   9\n",
      "\n",
      "Chi-square: sex vs. anxiety_level\n",
      "  Chi2 Statistic: 16.477\n",
      "  p-value: 0.001\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and sex.\n",
      "  Cramer's V: 0.184\n",
      "\n",
      "Contingency Table: anxiety_level vs. education_level\n",
      "education_level   0   1  2\n",
      "anxiety_level             \n",
      "0                25  72  3\n",
      "1                19  76  5\n",
      "2                20  78  2\n",
      "3                14  85  1\n",
      "\n",
      "Chi-square: education_level vs. anxiety_level\n",
      "  Chi2 Statistic: 7.452\n",
      "  p-value: 0.281\n",
      "  Degrees of freedom: 6\n",
      "  Result: No significant association between anxiety_level and education_level.\n",
      "\n",
      "Contingency Table: anxiety_level vs. shift_afternoon\n",
      "shift_afternoon  False  True \n",
      "anxiety_level                \n",
      "0                   63     37\n",
      "1                   72     28\n",
      "2                   87     13\n",
      "3                   77     23\n",
      "\n",
      "Chi-square: shift_afternoon vs. anxiety_level\n",
      "  Chi2 Statistic: 15.934\n",
      "  p-value: 0.001\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and shift_afternoon.\n",
      "  Cramer's V: 0.180\n",
      "\n",
      "Contingency Table: anxiety_level vs. shift_morning\n",
      "shift_morning  False  True \n",
      "anxiety_level              \n",
      "0                 72     28\n",
      "1                 71     29\n",
      "2                 65     35\n",
      "3                 94      6\n",
      "\n",
      "Chi-square: shift_morning vs. anxiety_level\n",
      "  Chi2 Statistic: 26.220\n",
      "  p-value: 0.000\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and shift_morning.\n",
      "  Cramer's V: 0.241\n",
      "\n",
      "Contingency Table: anxiety_level vs. shift_night a\n",
      "shift_night a  False  True \n",
      "anxiety_level              \n",
      "0                 92      8\n",
      "1                 91      9\n",
      "2                 89     11\n",
      "3                100      0\n",
      "\n",
      "Chi-square: shift_night a vs. anxiety_level\n",
      "  Chi2 Statistic: 10.753\n",
      "  p-value: 0.013\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and shift_night a.\n",
      "  Cramer's V: 0.139\n",
      "\n",
      "Contingency Table: anxiety_level vs. shift_night b\n",
      "shift_night b  False  True \n",
      "anxiety_level              \n",
      "0                 86     14\n",
      "1                 93      7\n",
      "2                 98      2\n",
      "3                 98      2\n",
      "\n",
      "Chi-square: shift_night b vs. anxiety_level\n",
      "  Chi2 Statistic: 16.512\n",
      "  p-value: 0.001\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and shift_night b.\n",
      "  Cramer's V: 0.184\n",
      "\n",
      "Contingency Table: anxiety_level vs. marital_domestic partnership\n",
      "marital_domestic partnership  False  True \n",
      "anxiety_level                             \n",
      "0                                86     14\n",
      "1                                87     13\n",
      "2                                91      9\n",
      "3                               100      0\n",
      "\n",
      "Chi-square: marital_domestic partnership vs. anxiety_level\n",
      "  Chi2 Statistic: 14.896\n",
      "  p-value: 0.002\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and marital_domestic partnership.\n",
      "  Cramer's V: 0.173\n",
      "\n",
      "Contingency Table: anxiety_level vs. marital_married\n",
      "marital_married  False  True \n",
      "anxiety_level                \n",
      "0                   67     33\n",
      "1                   58     42\n",
      "2                   76     24\n",
      "3                   88     12\n",
      "\n",
      "Chi-square: marital_married vs. anxiety_level\n",
      "  Chi2 Statistic: 24.577\n",
      "  p-value: 0.000\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and marital_married.\n",
      "  Cramer's V: 0.233\n",
      "\n",
      "Contingency Table: anxiety_level vs. marital_single\n",
      "marital_single  False  True \n",
      "anxiety_level               \n",
      "0                  56     44\n",
      "1                  72     28\n",
      "2                  86     14\n",
      "3                  63     37\n",
      "\n",
      "Chi-square: marital_single vs. anxiety_level\n",
      "  Chi2 Statistic: 23.610\n",
      "  p-value: 0.000\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and marital_single.\n",
      "  Cramer's V: 0.227\n",
      "\n",
      "Contingency Table: anxiety_level vs. category_gen nurse\n",
      "category_gen nurse  False  True \n",
      "anxiety_level                   \n",
      "0                      55     45\n",
      "1                      41     59\n",
      "2                      57     43\n",
      "3                      76     24\n",
      "\n",
      "Chi-square: category_gen nurse vs. anxiety_level\n",
      "  Chi2 Statistic: 25.363\n",
      "  p-value: 0.000\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and category_gen nurse.\n",
      "  Cramer's V: 0.237\n",
      "\n",
      "Contingency Table: anxiety_level vs. category_nurse aux\n",
      "category_nurse aux  False  True \n",
      "anxiety_level                   \n",
      "0                      56     44\n",
      "1                      75     25\n",
      "2                      75     25\n",
      "3                      81     19\n",
      "\n",
      "Chi-square: category_nurse aux vs. anxiety_level\n",
      "  Chi2 Statistic: 17.502\n",
      "  p-value: 0.001\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and category_nurse aux.\n",
      "  Cramer's V: 0.191\n",
      "\n",
      "Contingency Table: anxiety_level vs. category_other\n",
      "category_other  False  True \n",
      "anxiety_level               \n",
      "0                  96      4\n",
      "1                  92      8\n",
      "2                  95      5\n",
      "3                 100      0\n",
      "\n",
      "Chi-square: category_other vs. anxiety_level\n",
      "  Chi2 Statistic: 8.048\n",
      "  p-value: 0.045\n",
      "  Degrees of freedom: 3\n",
      "  Result: Significant association between anxiety_level and category_other.\n",
      "  Cramer's V: 0.112\n"
     ]
    }
   ],
   "source": [
    "inferential_analysis(df_augmented, \"anxiety_level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis: Key Conclusions\n",
    "\n",
    "This section summarizes the key findings from the statistical analysis, building upon the exploratory data analysis, correlation analysis, and incorporating results from the inferential tests *after including interaction terms*. We used non-parametric hypothesis tests (Kruskal-Wallis and Chi-square) to assess the statistical significance of relationships between the ordinal `anxiety_level` variable and other predictors, including interaction terms.\n",
    "\n",
    "**Main Conclusions:**\n",
    "\n",
    "*   **Significant Associations with Anxiety:** Several demographic, work-related factors, *and interaction terms*, show statistically significant associations with nurses' anxiety levels during the COVID-19 pandemic (p < 0.05, after appropriate corrections for multiple comparisons). These factors include:\n",
    "\n",
    "    *   **Original Variables:**\n",
    "        *   `sex` (Kruskal-Wallis and Chi-square)\n",
    "        *   `age_range` (Kruskal-Wallis)\n",
    "        *   `seniority_range` (Kruskal-Wallis)\n",
    "        *   All `shift` variables (Chi-square: `shift_afternoon`, `shift_morning`, `shift_night a`, `shift_night b`)\n",
    "        *   All `marital_status` variables (Chi-square: `marital_domestic partnership`, `marital_married`, `marital_single`)\n",
    "        *   All `category` variables (Chi-square: `category_gen nurse`, `category_nurse aux`, `category_other`)\n",
    "\n",
    "    *   **Interaction Terms (Kruskal-Wallis):**\n",
    "        *   `age_x_seniority`\n",
    "        *   `age_x_education`\n",
    "        *   `seniority_x_education`\n",
    "        *   `sex_x_age`\n",
    "        *   `sex_x_seniority`\n",
    "        *   `sex_x_education`\n",
    "\n",
    "*   **Non-Significant Association with Education Level (Alone):**  `education_level`, *when considered independently*, did *not* show a statistically significant association with anxiety levels in either the Kruskal-Wallis or Chi-square tests.  This is consistent with the findings before the inclusion of interaction terms.  *However*, interaction terms *involving* education *do* show significant associations (see above).\n",
    "\n",
    "*   **Strength of Associations:** While many associations were statistically significant, it's crucial to remember that the Spearman correlation analysis generally showed *weak* correlations, and the Cramer's V values (from the Chi-square tests) were mostly in the low to moderate range.  This indicates that, while these factors are *related* to anxiety, no single factor is a *strong* predictor on its own. Anxiety is a complex, multi-factorial outcome.\n",
    "\n",
    "* **Importance of Interactions:** The significance of several interaction terms in the Kruskal-Wallis tests highlights that the relationship between these factors and anxiety is not always simple or additive.  The effect of one variable (e.g., age) on anxiety *depends* on the level of another variable (e.g., seniority or education).  This justifies the inclusion of interaction terms in the subsequent predictive modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using k_neighbors = 5 for SMOTE.\n",
      "Class distribution before SMOTE: Counter({0: 62, 1: 49, 2: 22, 3: 7})\n",
      "Class distribution after SMOTE: Counter({1: 100, 0: 100, 2: 100, 3: 100})\n",
      "--- Optimizing and Evaluating KNN ---\n",
      "Loaded RandomizedSearchCV for KNN from KNN_random_search.pkl\n",
      "Best parameters for KNN: {'knn__n_neighbors': 7, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "Training Accuracy: 0.7031\n",
      "Test Accuracy: 0.6625\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13  4  2  1]\n",
      " [ 4 11  1  4]\n",
      " [ 1  3 12  4]\n",
      " [ 1  0  2 17]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.65      0.67        20\n",
      "           1       0.61      0.55      0.58        20\n",
      "           2       0.71      0.60      0.65        20\n",
      "           3       0.65      0.85      0.74        20\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.66      0.66      0.66        80\n",
      "weighted avg       0.66      0.66      0.66        80\n",
      "\n",
      "--- Optimizing and Evaluating Decision Tree ---\n",
      "Loaded RandomizedSearchCV for Decision Tree from Decision Tree_random_search.pkl\n",
      "Best parameters for Decision Tree: {'criterion': 'log_loss', 'max_depth': 133, 'min_samples_leaf': 1, 'min_samples_split': 6}\n",
      "Training Accuracy: 0.8375\n",
      "Test Accuracy: 0.6875\n",
      "\n",
      "Confusion Matrix:\n",
      "[[17  2  1  0]\n",
      " [ 7  9  3  1]\n",
      " [ 5  1 13  1]\n",
      " [ 2  0  2 16]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.85      0.67        20\n",
      "           1       0.75      0.45      0.56        20\n",
      "           2       0.68      0.65      0.67        20\n",
      "           3       0.89      0.80      0.84        20\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.72      0.69      0.68        80\n",
      "weighted avg       0.72      0.69      0.68        80\n",
      "\n",
      "--- Optimizing and Evaluating Random Forest ---\n",
      "Loaded RandomizedSearchCV for Random Forest from Random Forest_random_search.pkl\n",
      "Best parameters for Random Forest: {'bootstrap': True, 'max_depth': 22, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 3823}\n",
      "Training Accuracy: 0.8500\n",
      "Test Accuracy: 0.6875\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15  3  2  0]\n",
      " [ 5 10  3  2]\n",
      " [ 5  1 13  1]\n",
      " [ 1  0  2 17]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65        20\n",
      "           1       0.71      0.50      0.59        20\n",
      "           2       0.65      0.65      0.65        20\n",
      "           3       0.85      0.85      0.85        20\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.70      0.69      0.69        80\n",
      "weighted avg       0.70      0.69      0.69        80\n",
      "\n",
      "--- Optimizing and Evaluating AdaBoost ---\n",
      "Loaded RandomizedSearchCV for AdaBoost from AdaBoost_random_search.pkl\n",
      "Best parameters for AdaBoost: {'learning_rate': np.float64(0.5972700559185838), 'n_estimators': 51}\n",
      "Training Accuracy: 0.4875\n",
      "Test Accuracy: 0.4625\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 4  7  5  4]\n",
      " [ 3  5 11  1]\n",
      " [ 1  6 12  1]\n",
      " [ 0  0  4 16]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.20      0.29        20\n",
      "           1       0.28      0.25      0.26        20\n",
      "           2       0.38      0.60      0.46        20\n",
      "           3       0.73      0.80      0.76        20\n",
      "\n",
      "    accuracy                           0.46        80\n",
      "   macro avg       0.47      0.46      0.44        80\n",
      "weighted avg       0.47      0.46      0.44        80\n",
      "\n",
      "--- Optimizing and Evaluating Gradient Boosting ---\n",
      "No checkpoint found for Gradient Boosting. Starting RandomizedSearchCV.\n",
      "Type of param_dist: <class 'dict'>\n",
      "Contents of param_dist: {'gb__n_estimators': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025A39AB2C50>, 'gb__learning_rate': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000025A3A82A190>, 'gb__max_depth': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025A3A828B50>, 'gb__min_samples_split': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025A3A82A1D0>, 'gb__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025A3A82AED0>, 'gb__subsample': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000025A3A829B90>, 'gb__max_features': [None, 'sqrt', 'log2']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning:\n",
      "\n",
      "\n",
      "135 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.0774801263571896) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.1664580622368363) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.073931655089634) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.3537614045478823) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.0879973262260967) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.1300178274831856) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.375667702211647) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.0317537059112638) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.1100459770841704) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.208628038893518) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.204440910834439) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.2464963036515337) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.0085978420620525) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.3774256661767685) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.1445643612438134) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.11613823232756) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.1405520936314246) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.1279872368273431) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.046634762885678) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.1807645430961915) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.3521067539662228) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.0546468826683773) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.2913340520575225) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.3426548500701805) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.3305924420451771) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.1615735018061626) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'subsample' parameter of GradientBoostingClassifier must be a float in the range (0.0, 1.0]. Got np.float64(1.1376209082361595) instead.\n",
      "\n",
      "\n",
      "C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [     nan      nan 0.671875 0.621875      nan      nan      nan      nan\n",
      "      nan 0.31875       nan 0.246875 0.309375 0.653125 0.64375       nan\n",
      "      nan 0.634375      nan      nan 0.3125        nan 0.634375      nan\n",
      " 0.315625      nan 0.475    0.653125 0.3375        nan      nan      nan\n",
      "      nan      nan 0.375         nan 0.425    0.425         nan 0.63125\n",
      "      nan 0.66875       nan 0.653125 0.65625  0.5875   0.64375       nan\n",
      "      nan      nan]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV for Gradient Boosting saved to Gradient Boosting_random_search.pkl\n",
      "Best parameters for Gradient Boosting: {'gb__learning_rate': np.float64(0.011292247147901224), 'gb__max_depth': 4, 'gb__max_features': 'sqrt', 'gb__min_samples_leaf': 6, 'gb__min_samples_split': 3, 'gb__n_estimators': 1284, 'gb__subsample': np.float64(0.8433937943676302)}\n",
      "Training Accuracy: 0.8719\n",
      "Test Accuracy: 0.7250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[17  2  1  0]\n",
      " [ 5 10  3  2]\n",
      " [ 2  4 13  1]\n",
      " [ 1  0  1 18]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.85      0.76        20\n",
      "           1       0.62      0.50      0.56        20\n",
      "           2       0.72      0.65      0.68        20\n",
      "           3       0.86      0.90      0.88        20\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.72      0.72      0.72        80\n",
      "weighted avg       0.72      0.72      0.72        80\n",
      "\n",
      "--- Optimizing and Evaluating XGBoost ---\n",
      "No checkpoint found for XGBoost. Starting RandomizedSearchCV.\n",
      "Type of param_dist: <class 'dict'>\n",
      "Contents of param_dist: {'xgb__n_estimators': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025A32ED4990>, 'xgb__learning_rate': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000025A32ED5710>, 'xgb__max_depth': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025A32ED4E90>, 'xgb__subsample': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000025A32ED7E90>, 'xgb__colsample_bytree': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000025A32ED5810>, 'xgb__gamma': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000025A32ED68D0>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning:\n",
      "\n",
      "[09:33:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV for XGBoost saved to XGBoost_random_search.pkl\n",
      "Best parameters for XGBoost: {'xgb__colsample_bytree': np.float64(0.7670604991178476), 'xgb__gamma': np.float64(0.16039003248586792), 'xgb__learning_rate': np.float64(0.11291110623991253), 'xgb__max_depth': 31, 'xgb__n_estimators': 1894, 'xgb__subsample': np.float64(0.7181785886376484)}\n",
      "Training Accuracy: 0.9000\n",
      "Test Accuracy: 0.7125\n",
      "\n",
      "Confusion Matrix:\n",
      "[[16  2  2  0]\n",
      " [ 6 11  2  1]\n",
      " [ 5  1 13  1]\n",
      " [ 2  0  1 17]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.80      0.65        20\n",
      "           1       0.79      0.55      0.65        20\n",
      "           2       0.72      0.65      0.68        20\n",
      "           3       0.89      0.85      0.87        20\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.74      0.71      0.71        80\n",
      "weighted avg       0.74      0.71      0.71        80\n",
      "\n",
      "--- Optimizing and Evaluating Extra Trees ---\n",
      "No checkpoint found for Extra Trees. Starting RandomizedSearchCV.\n",
      "Type of param_dist: <class 'dict'>\n",
      "Contents of param_dist: {'n_estimators': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025A32ED7A10>, 'max_depth': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025A32ED5B50>, 'min_samples_split': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025A3A5E48D0>, 'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025A3A5E6D50>, 'max_features': [None, 'sqrt', 'log2'], 'bootstrap': [True, False]}\n",
      "RandomizedSearchCV for Extra Trees saved to Extra Trees_random_search.pkl\n",
      "Best parameters for Extra Trees: {'bootstrap': True, 'max_depth': 22, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 3823}\n",
      "Training Accuracy: 0.8500\n",
      "Test Accuracy: 0.7000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[16  3  1  0]\n",
      " [ 5 10  3  2]\n",
      " [ 5  1 13  1]\n",
      " [ 1  0  2 17]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.80      0.68        20\n",
      "           1       0.71      0.50      0.59        20\n",
      "           2       0.68      0.65      0.67        20\n",
      "           3       0.85      0.85      0.85        20\n",
      "\n",
      "    accuracy                           0.70        80\n",
      "   macro avg       0.71      0.70      0.70        80\n",
      "weighted avg       0.71      0.70      0.70        80\n",
      "\n",
      "\n",
      "--- Model Comparison ---\n",
      "               Model  Test Accuracy\n",
      "4  Gradient Boosting         0.7250\n",
      "5            XGBoost         0.7125\n",
      "6        Extra Trees         0.7000\n",
      "1      Decision Tree         0.6875\n",
      "2      Random Forest         0.6875\n",
      "0                KNN         0.6625\n",
      "3           AdaBoost         0.4625\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Model=%{y}<br>Test Accuracy=%{x}<extra></extra>",
         "legendgroup": "Gradient Boosting",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Gradient Boosting",
         "orientation": "h",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": {
          "bdata": "MzMzMzMz5z8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": [
          "Gradient Boosting"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Model=%{y}<br>Test Accuracy=%{x}<extra></extra>",
         "legendgroup": "XGBoost",
         "marker": {
          "color": "#EF553B",
          "pattern": {
           "shape": ""
          }
         },
         "name": "XGBoost",
         "orientation": "h",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": {
          "bdata": "zczMzMzM5j8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": [
          "XGBoost"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Model=%{y}<br>Test Accuracy=%{x}<extra></extra>",
         "legendgroup": "Extra Trees",
         "marker": {
          "color": "#00cc96",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Extra Trees",
         "orientation": "h",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": {
          "bdata": "ZmZmZmZm5j8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": [
          "Extra Trees"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Model=%{y}<br>Test Accuracy=%{x}<extra></extra>",
         "legendgroup": "Decision Tree",
         "marker": {
          "color": "#ab63fa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Decision Tree",
         "orientation": "h",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": {
          "bdata": "AAAAAAAA5j8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": [
          "Decision Tree"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Model=%{y}<br>Test Accuracy=%{x}<extra></extra>",
         "legendgroup": "Random Forest",
         "marker": {
          "color": "#FFA15A",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Random Forest",
         "orientation": "h",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": {
          "bdata": "AAAAAAAA5j8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": [
          "Random Forest"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Model=%{y}<br>Test Accuracy=%{x}<extra></extra>",
         "legendgroup": "KNN",
         "marker": {
          "color": "#19d3f3",
          "pattern": {
           "shape": ""
          }
         },
         "name": "KNN",
         "orientation": "h",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": {
          "bdata": "MzMzMzMz5T8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": [
          "KNN"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Model=%{y}<br>Test Accuracy=%{x}<extra></extra>",
         "legendgroup": "AdaBoost",
         "marker": {
          "color": "#FF6692",
          "pattern": {
           "shape": ""
          }
         },
         "name": "AdaBoost",
         "orientation": "h",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": {
          "bdata": "mpmZmZmZ3T8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": [
          "AdaBoost"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "title": {
          "text": "Model"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Model Comparison (Test Accuracy)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Test Accuracy"
         }
        },
        "yaxis": {
         "anchor": "x",
         "categoryarray": [
          "AdaBoost",
          "KNN",
          "Random Forest",
          "Decision Tree",
          "Extra Trees",
          "XGBoost",
          "Gradient Boosting"
         ],
         "categoryorder": "total ascending",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Model"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    # --- FEATURE ENGINEERING \n",
    "    df_encoded = encode_variables(df)  \n",
    "\n",
    "    # --- SMOTE and train_test_split AFTER feature engineering ---\n",
    "    X = df_encoded.drop('anxiety_level', axis=1)\n",
    "    y = df_encoded['anxiety_level']\n",
    "\n",
    "    X_resampled, y_resampled = augment_data_smote(df_encoded, 'anxiety_level', sampling_strategy={0: 100, 1: 100, 2: 100, 3: 100}, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "   \n",
    "    # 3. Create models and parameter distributions\n",
    "    models = create_models_dict()\n",
    "    param_distributions = create_param_distributions()\n",
    "\n",
    "    # --- Model Optimization and Evaluation Loop ---\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"--- Optimizing and Evaluating {name} ---\")\n",
    "        if name in param_distributions:\n",
    "            # Train and evaluate using the FULL feature set (X_train, X_test)\n",
    "            test_accuracy = optimize_and_evaluate(name, model, param_distributions[name], X_train, y_train, X_test, y_test, n_iter= 50) # Aumentamos el numero de iteraciones\n",
    "            if test_accuracy is not None:\n",
    "                results[name] = test_accuracy\n",
    "        else:\n",
    "            print(f\"No parameter distributions found for {name}.  Training without optimization.\")\n",
    "            trained_model = train_model(model, X_train, y_train)\n",
    "            test_accuracy = evaluate_model(trained_model, X_train, y_train, X_test, y_test)\n",
    "            results[name] = test_accuracy\n",
    "\n",
    "     # --- Model Comparison (with Plotly) ---\n",
    "    print(\"\\n--- Model Comparison ---\")\n",
    "    results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Test Accuracy'])\n",
    "    results_df = results_df.sort_values(by='Test Accuracy', ascending=False)  # Sort before plotting\n",
    "    print(results_df)\n",
    "\n",
    "    # Create the bar chart with Plotly\n",
    "    fig = px.bar(results_df, x='Test Accuracy', y='Model',\n",
    "                title='Model Comparison (Test Accuracy)',\n",
    "                orientation='h',  # Horizontal bar chart\n",
    "                color='Model',  # Color bars by model\n",
    "                )\n",
    "    fig.update_layout(yaxis={'categoryorder':'total ascending'}) #Sort\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using k_neighbors = 5 for SMOTE.\n",
      "Class distribution before SMOTE: Counter({0: 62, 1: 49, 2: 22, 3: 7})\n",
      "Class distribution after SMOTE: Counter({1: 100, 0: 100, 2: 100, 3: 100})\n",
      "\n",
      "Comparison Table (Real vs. Predicted):\n",
      "     Index  Real  Predicted\n",
      "170    170     0          0\n",
      "259    259     2          2\n",
      "262    262     2          2\n",
      "224    224     1          1\n",
      "5        5     2          1\n",
      "119    119     0          0\n",
      "9        9     1          1\n",
      "276    276     2          2\n",
      "275    275     2          2\n",
      "360    360     3          3\n",
      "358    358     3          2\n",
      "261    261     2          2\n",
      "318    318     3          3\n",
      "326    326     3          3\n",
      "81      81     0          0\n",
      "329    329     3          3\n",
      "226    226     1          1\n",
      "12      12     0          1\n",
      "101    101     2          0\n",
      "162    162     0          0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    df_encoded = encode_variables(df)\n",
    "    X = df_encoded.drop('anxiety_level', axis=1)\n",
    "    y = df_encoded['anxiety_level']\n",
    "    X_resampled, y_resampled = augment_data_smote(df_encoded, 'anxiety_level',\n",
    "                                                sampling_strategy={0: 100, 1: 100, 2: 100, 3: 100},\n",
    "                                                random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,\n",
    "                                                    test_size=0.2, random_state=42,\n",
    "                                                    stratify=y_resampled)\n",
    "\n",
    "    # --- Train Gradient Boosting (with the best hyperparameters) ---\n",
    "    best_gb_params = {\n",
    "        'n_estimators': 1284,\n",
    "        'learning_rate': 0.011292247147901224,\n",
    "        'max_depth': 4,\n",
    "        'subsample': 0.8433937943676302,\n",
    "        'max_features': 'sqrt',\n",
    "        'min_samples_split': 3,\n",
    "        'min_samples_leaf': 6,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "\n",
    "    gb_model = GradientBoostingClassifier(**best_gb_params)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "\n",
    "    # --- Prepare data for visualization ---\n",
    "\n",
    "    # 1. Take a sample of 20 nurses (from the test set)\n",
    "    sample_size = 20\n",
    "    X_sample = X_test.sample(sample_size, random_state=42)\n",
    "    y_sample = y_test.loc[X_sample.index]  # Use .loc to avoid SettingWithCopyWarning\n",
    "\n",
    "    # 2. Get the model's predictions for the sample\n",
    "    y_pred_sample = gb_model.predict(X_sample)\n",
    "\n",
    "    # 3. Create a DataFrame for visualization\n",
    "    plot_df = pd.DataFrame({\n",
    "        'Index': X_sample.index,  # Use original indices\n",
    "        'Real': y_sample,\n",
    "        'Predicted': y_pred_sample\n",
    "    })\n",
    "\n",
    "    # 4. Convert to \"long\" format to facilitate plotting with Seaborn\n",
    "    plot_df_long = plot_df.melt(id_vars='Index', value_vars=['Real', 'Predicted'],\n",
    "                                var_name='Type', value_name='Anxiety Level')\n",
    "\n",
    "    # --- Create the plot ---\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x='Index', y='Anxiety Level', hue='Type', data=plot_df_long,\n",
    "                palette={\"Real\": \"skyblue\", \"Predicted\": \"salmon\"})\n",
    "\n",
    "    plt.title('Comparison of Real vs. Predicted Anxiety Levels (Gradient Boosting) - Sample of 20 Nurses')\n",
    "    plt.xlabel('Nurse Index (Sample)')\n",
    "    plt.ylabel('Anxiety Level')\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels\n",
    "    plt.tight_layout()\n",
    "    plt.legend(title='Type')\n",
    "    plt.show()\n",
    "\n",
    "    # --- (Optional) Print the comparison table ---\n",
    "    print(\"\\nComparison Table (Real vs. Predicted):\")\n",
    "    print(plot_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
